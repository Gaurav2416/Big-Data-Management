{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_Emotion(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Deep_Emotion class contains the network architecture.\n",
    "        '''\n",
    "        super(Deep_Emotion,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,10,3)\n",
    "        self.conv2 = nn.Conv2d(10,10,3)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(10,10,3)\n",
    "        self.conv4 = nn.Conv2d(10,10,3)\n",
    "        self.pool4 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.norm = nn.BatchNorm2d(10)\n",
    "\n",
    "        self.fc1 = nn.Linear(810,50)\n",
    "        self.fc2 = nn.Linear(50,7)\n",
    "\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=7),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(640, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "    def stn(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 640)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        grid = fn.affine_grid(theta, x.size())\n",
    "        x = fn.grid_sample(x, grid)\n",
    "        return x\n",
    "\n",
    "    def forward(self,input):\n",
    "        out = self.stn(input)\n",
    "\n",
    "        out = fn.relu(self.conv1(out))\n",
    "        out = self.conv2(out)\n",
    "        out = fn.relu(self.pool2(out))\n",
    "\n",
    "        out = fn.relu(self.conv3(out))\n",
    "        out = self.norm(self.conv4(out))\n",
    "        out = fn.relu(self.pool4(out))\n",
    "\n",
    "        out = fn.dropout(out)\n",
    "        out = out.view(-1, 810)\n",
    "        out = fn.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 48 * 48  2304 =  1D dimension save in csv along with label\n",
    "class Generate_data():\n",
    "    def __init__(self, datapath):\n",
    "        \"\"\"\n",
    "        Generate_data class\n",
    "        Two methods to be used\n",
    "        1-split_test\n",
    "        2-save_images\n",
    "        [Note] that you have to split the public and private from fer2013 file\n",
    "        \"\"\"\n",
    "        self.data_path = datapath\n",
    "\n",
    "    def split_test(self, test_filename = 'finaltest', val_filename= 'val'):\n",
    "        \"\"\"\n",
    "        Helper function to split the validation and test data from general test file as it contains (Public test, Private test)\n",
    "            params:-\n",
    "                data_path = path to the folder that contains the test data file\n",
    "        \"\"\"\n",
    "        csv_path = self.data_path +\"/\"+ 'test.csv'\n",
    "        test = pd.read_csv(csv_path)\n",
    "        validation_data = pd.DataFrame(test.iloc[:3589,:])\n",
    "        test_data = pd.DataFrame(test.iloc[3589:,:])\n",
    "        test_data.to_csv(self.data_path+\"/\"+test_filename+\".csv\")\n",
    "        validation_data.to_csv(self.data_path+\"/\"+val_filename+\".csv\")\n",
    "        print(\"Done splitting the test file into validation & final test file\")\n",
    "\n",
    "    def str_to_image(self, str_img = ' '):\n",
    "        '''\n",
    "        Convert string pixels from the csv file into image object\n",
    "            params:- take an image string\n",
    "            return :- return PIL image object\n",
    "        '''\n",
    "        imgarray_str = str_img.split(' ')\n",
    "        imgarray = np.asarray(imgarray_str,dtype=np.uint8).reshape(48,48)\n",
    "        return Image.fromarray(imgarray)\n",
    "\n",
    "    def save_images(self, datatype='train'):\n",
    "        '''\n",
    "        save_images is a function responsible for saving images from data files e.g(train, test) in a desired folder\n",
    "            params:-\n",
    "            datatype= str e.g (train, val, finaltest)\n",
    "        '''\n",
    "        foldername= self.data_path+\"/\"+datatype\n",
    "        csvfile_path= self.data_path+\"/\"+datatype+'.csv'\n",
    "        if not os.path.exists(foldername):\n",
    "            os.mkdir(foldername)\n",
    "\n",
    "        data = pd.read_csv(csvfile_path)\n",
    "        images = data['pixels'] #dataframe to series pandas\n",
    "        numberofimages = images.shape[0]\n",
    "        for index in tqdm(range(numberofimages)):\n",
    "            img = self.str_to_image(images[index])\n",
    "            img.save(os.path.join(foldername,'{}{}.jpg'.format(datatype,index)),'JPEG')\n",
    "        print('Done saving {} data'.format((foldername)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib as plt\n",
    "\n",
    "class Plain_Dataset(Dataset):\n",
    "    def __init__(self,csv_file,img_dir,datatype,transform):\n",
    "        '''\n",
    "        Pytorch Dataset class\n",
    "        params:-\n",
    "                 csv_file : the path of the csv file    (train, validation, test)\n",
    "                 img_dir  : the directory of the images (train, validation, test)\n",
    "                 datatype : string for searching along the image_dir (train, val, test)\n",
    "                 transform: pytorch transformation over the data\n",
    "        return :-\n",
    "                 image, labels\n",
    "        '''\n",
    "        self.csv_file = pd.read_csv(csv_file)\n",
    "        self.lables = self.csv_file['emotion']\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.datatype = datatype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv_file)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img = Image.open(self.img_dir+self.datatype+str(idx)+'.jpg')\n",
    "        lables = np.array(self.lables[idx])\n",
    "        lables = torch.from_numpy(lables).long()\n",
    "\n",
    "        if self.transform :\n",
    "            img = self.transform(img)\n",
    "        return img,lables\n",
    "\n",
    "#Helper function\n",
    "def eval_data_dataloader(csv_file,img_dir,datatype,sample_number,transform= None):\n",
    "    '''\n",
    "    Helper function used to evaluate the Dataset class\n",
    "    params:-\n",
    "            csv_file : the path of the csv file    (train, validation, test)\n",
    "            img_dir  : the directory of the images (train, validation, test)\n",
    "            datatype : string for searching along the image_dir (train, val, test)\n",
    "            sample_number : any number from the data to be shown\n",
    "    '''\n",
    "    if transform is None :\n",
    "        transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
    "    dataset = Plain_Dataset(csv_file=csv_file,img_dir = img_dir,datatype = datatype,transform = transform)\n",
    "\n",
    "    label = dataset.__getitem__(sample_number)[1]\n",
    "    print(label)\n",
    "    imgg = dataset.__getitem__(sample_number)[0]\n",
    "    imgnumpy = imgg.numpy()\n",
    "    imgt = imgnumpy.squeeze()\n",
    "    plt.imshow(imgt)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-s SETUP] -d DATA [-hparams HYPERPARAMS]\n",
      "                             [-e EPOCHS] [-lr LEARNING_RATE] [-bs BATCH_SIZE]\n",
      "                             [-t TRAIN]\n",
      "ipykernel_launcher.py: error: the following arguments are required: -d/--data\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import numpy  as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "# from data_loaders import Plain_Dataset, eval_data_dataloader\n",
    "# from deep_emotion import Deep_Emotion\n",
    "# from generate_data import Generate_data\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def Train(epochs,train_loader,val_loader,criterion,optmizer,device):\n",
    "    '''\n",
    "    Training Loop\n",
    "    '''\n",
    "    print(\"===================================Start Training===================================\")\n",
    "    for e in range(epochs):\n",
    "        train_loss = 0\n",
    "        validation_loss = 0\n",
    "        train_correct = 0\n",
    "        val_correct = 0\n",
    "        # Train the model  #\n",
    "        net.train()\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optmizer.zero_grad()\n",
    "            outputs = net(data)\n",
    "            loss = criterion(outputs,labels)\n",
    "            loss.backward()\n",
    "            optmizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(outputs,1)\n",
    "            train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        #validate the model#\n",
    "        net.eval()\n",
    "        for data,labels in val_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            val_outputs = net(data)\n",
    "            val_loss = criterion(val_outputs, labels)\n",
    "            validation_loss += val_loss.item()\n",
    "            _, val_preds = torch.max(val_outputs,1)\n",
    "            val_correct += torch.sum(val_preds == labels.data)\n",
    "\n",
    "        train_loss = train_loss/len(train_dataset)\n",
    "        train_acc = train_correct.double() / len(train_dataset)\n",
    "        validation_loss =  validation_loss / len(validation_dataset)\n",
    "        val_acc = val_correct.double() / len(validation_dataset)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.8f} \\tValidation Loss {:.8f} \\tTraining Acuuarcy {:.3f}% \\tValidation Acuuarcy {:.3f}%'\n",
    "                                                           .format(e+1, train_loss,validation_loss,train_acc * 100, val_acc*100))\n",
    "\n",
    "    torch.save(net.state_dict(),'deep_emotion-{}-{}-{}.pt'.format(epochs,batchsize,lr))\n",
    "    print(\"===================================Training Finished===================================\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description=\"Configuration of setup and training process\")\n",
    "    parser.add_argument('-s', '--setup', type=bool, help='setup the dataset for the first time')\n",
    "    parser.add_argument('-d', '--data', type=str,required= True,\n",
    "                               help='data folder that contains data files that downloaded from kaggle (train.csv and test.csv)')\n",
    "    parser.add_argument('-hparams', '--hyperparams', type=bool,\n",
    "                               help='True when changing the hyperparameters e.g (batch size, LR, num. of epochs)')\n",
    "    parser.add_argument('-e', '--epochs', type= int, help= 'number of epochs')\n",
    "    parser.add_argument('-lr', '--learning_rate', type= float, help= 'value of learning rate')\n",
    "    parser.add_argument('-bs', '--batch_size', type= int, help= 'training/validation batch size')\n",
    "    parser.add_argument('-t', '--train', type=bool, help='True when training')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.setup :\n",
    "        generate_dataset = Generate_data(args.data)\n",
    "        generate_dataset.split_test()\n",
    "        generate_dataset.save_images()\n",
    "        generate_dataset.save_images('finaltest')\n",
    "        generate_dataset.save_images('val')\n",
    "\n",
    "    if args.hyperparams:\n",
    "        epochs = args.epochs\n",
    "        lr = args.learning_rate\n",
    "        batchsize = args.batch_size\n",
    "    else :\n",
    "        epochs = 100\n",
    "        lr = 0.005\n",
    "        batchsize = 128\n",
    "\n",
    "    if args.train:\n",
    "        net = Deep_Emotion()\n",
    "        net.to(device)\n",
    "        print(\"Model archticture: \", net)\n",
    "        traincsv_file = args.data+'/'+'train.csv'\n",
    "        validationcsv_file = args.data+'/'+'val.csv'\n",
    "        train_img_dir = args.data+'/'+'train/'\n",
    "        validation_img_dir = args.data+'/'+'val/'\n",
    "\n",
    "        transformation= transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
    "        train_dataset= Plain_Dataset(csv_file=traincsv_file, img_dir = train_img_dir, datatype = 'train', transform = transformation)\n",
    "        validation_dataset= Plain_Dataset(csv_file=validationcsv_file, img_dir = validation_img_dir, datatype = 'val', transform = transformation)\n",
    "        train_loader= DataLoader(train_dataset,batch_size=batchsize,shuffle = True,num_workers=0)\n",
    "        val_loader=   DataLoader(validation_dataset,batch_size=batchsize,shuffle = True,num_workers=0)\n",
    "\n",
    "        criterion= nn.CrossEntropyLoss()\n",
    "        optmizer= optim.Adam(net.parameters(),lr= lr)\n",
    "        Train(epochs, train_loader, val_loader, criterion, optmizer, device)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
